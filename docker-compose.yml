version: "3.8"

services:

  nessie:
    image: projectnessie/nessie:latest
    container_name: nessie
    user: "0:0" # <--- ADD THIS LINE (0:0 means Root User : Root Group)
    ports:
      - "19120:19120"
    environment:
      NESSIE_VERSION_STORE_TYPE: ROCKSDB
      NESSIE_VERSION_STORE_PERSIST_ROCKS_DATABASE_PATH: /var/nessie
      JAVA_TOOL_OPTIONS: "-Dnessie.server.authorization.enabled=false -Xmx1G"
      QUARKUS_OPENTELEMETRY_ENABLED: "false"
    restart: unless-stopped
    volumes:
      - nessie_data:/var/nessie
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:19120/api/v2/trees/- || exit 1" ]
      interval: 10s
      timeout: 5s
      retries: 8
      start_period: 60s
    networks:
      - lakehouse-net
  # nessie:
  #   image: projectnessie/nessie:latest
  #   container_name: nessie
  #   ports:
  #     - "19120:19120"
  #   environment:
  #     JAVA_TOOL_OPTIONS: "-Dnessie.server.authorization.enabled=false -Xmx1G"
  #     QUARKUS_OPENTELEMETRY_ENABLED: "false"
  #   restart: unless-stopped
  #   volumes:
  #     - nessie_data:/var/nessie
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:19120/api/v2/trees/- || exit 1"]
  #     interval: 10s
  #     timeout: 5s
  #     retries: 8
  #     start_period: 60s
  #   networks:
  #     - lakehouse-net

  # ---------------------------
  # PostgreSQL (Airflow metadata)
  # ---------------------------
  postgres:
    image: postgres:15
    container_name: postgres
    restart: unless-stopped
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: sirrus_ai
    ports:
      - "5435:5432"
    volumes:
      - pg_data:/var/lib/postgresql/data
    networks:
      - lakehouse-net

  # ---------------------------
  # Airflow Init
  # ---------------------------
  airflow-init:
    build: ./airflow
    depends_on:
      - postgres
    env_file:
      - .env
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/data:/opt/airflow/data
      - ./jars:/opt/airflow/jars
      - ./spark_jobs:/opt/airflow/spark_jobs
    command: >
      bash -c "airflow db init && airflow users create --username airflow --firstname Airflow --lastname Admin --role Admin --email admin@example.com --password airflow"
    networks:
      - lakehouse-net

  # ---------------------------
  # Airflow Webserver
  # ---------------------------
  airflow-webserver:
    build: ./airflow
    container_name: airflow-webserver
    depends_on:
      - postgres
      - airflow-init
    env_file:
      - .env
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/sirrus_ai
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/sirrus_ai
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__API__AUTH_BACKENDS: airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/data:/opt/airflow/data
      - ./jars:/opt/airflow/jars
      - ./spark_jobs:/opt/airflow/spark_jobs
    ports:
      - "8080:8080"
    command: webserver
    mem_limit: 4g
    networks:
      - lakehouse-net

  # ---------------------------
  # Airflow Scheduler
  # ---------------------------
  airflow-scheduler:
    build: ./airflow
    container_name: airflow-scheduler
    depends_on:
      - airflow-webserver
      - airflow-init
      - postgres
    env_file:
      - .env
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/sirrus_ai
      AIRFLOW__CORE__DEFAULT_TIMEZONE: "UTC"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/data:/opt/airflow/data
      - ./jars:/opt/airflow/jars
      - ./spark_jobs:/opt/airflow/spark_jobs
    command: scheduler
    mem_limit: 8g
    networks:
      - lakehouse-net

  # ---------------------------
  # Spark
  # ---------------------------
  spark:
    image: apache/spark:3.4.1-python3
    container_name: spark
    environment:
      - SPARK_MODE=master
    ports:
      - "8081:8081"
    volumes:
      - ./spark_jobs:/opt/spark/jobs
      - ./jars:/opt/jars
    depends_on:
      - nessie
    networks:
      - lakehouse-net

  # ---------------------------
  # Dremio
  # ---------------------------
  dremio:
    image: dremio/dremio-oss:24.3
    container_name: dremio
    restart: unless-stopped
    ports:
      - "9047:9047"
      - "31010:31010"
    environment:
      - DREMIO_JAVA_SERVER_EXTRA_OPTS=-Dnessie.enabled=true
    depends_on:
      - nessie
    networks:
      - lakehouse-net

volumes:
  pg_data:
  nessie_data:


networks:
  lakehouse-net:
    external: true
